{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUSIC RECOMMENDATIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas Page (Brainstorming)\n",
    "***\n",
    "## Data\n",
    "- [Vetting playlist](https://drive.google.com/file/d/1h_U-_NNFbMLVivUle-4ghbCpE_0pPBIJ/view)\n",
    "- [Million Playlist Data](https://drive.google.com/drive/folders/1pfpf2HkcksQveB-gg3XVQ5yPxDSY98u0)\n",
    "- The [Spotify Api](https://developer.spotify.com/documentation/web-api)'s\n",
    "- [Music Info Dataset](https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks)\n",
    "\n",
    "## [Analysis](https://drive.google.com/drive/folders/1IiB3BZoV2k9laW_eGmjv9jCp-LBhMHTZ)\n",
    "- We could cluster the music playlist data using the song data within it, and and try to categorize Mrs. Spindt into one of these playlists\n",
    "- We could also use the (unfortunatly deprecated) music info API\n",
    "\n",
    "## Variables\n",
    "We need to choose 4+ variables. The ? show which ones we need to decide on\n",
    "- Duration(ms)\n",
    "- Track Popularity(May convert to overall album popularity)\n",
    "- Release Date?\n",
    "- <s>Explict</s>\n",
    "- Artist?\n",
    "- <s>Available Markets</s>\n",
    "- Local?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "#import random\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "#import datetime\n",
    "import requests as rq\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "#import time\n",
    "#from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, svm \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def save_pickle(file, value):\n",
    "    pickle.dump(value, open(file, \"wb\"))\n",
    "def load_pickle(file):\n",
    "    return pickle.load(open(file, \"rb\"))\n",
    "\n",
    "# Load essential data\n",
    "playlist = json.load(open(\"data/vetting_playlist.json\"))['tracks']['items']\n",
    "musicInfo = pd.read_csv(\"data/tracks.csv\")\n",
    "\n",
    "oneMSongs = json.load(open(\"data/million_playlists/songs_0-999.json\"))\n",
    "print(oneMSongs)\n",
    "\n",
    "\n",
    "songs = []\n",
    "for x in playlist:\n",
    "    songs.append(x['track'])\n",
    "songs = pd.DataFrame(songs)\n",
    "\n",
    "\n",
    "def getSongInfo(idList):\n",
    "    songsNew = []\n",
    "    errors = 0\n",
    "    for x in idList:\n",
    "        try:\n",
    "            songsNew.append(musicInfo.iloc[list(musicInfo.loc[musicInfo['id'] == idList[x]].to_dict()['id'].keys())[0]])\n",
    "            # This massave hunk of code basically gets the row of the track ID and strips the dictionary to just get the raw value\n",
    "        except:\n",
    "            errors += 1\n",
    "            \n",
    "    errorRatio = int((errors / len(idList)) * 100) # This shows how much of the origional array was not found (the higher the worse)\n",
    "    return pd.DataFrame(songsNew), errorRatio\n",
    "\n",
    "\n",
    "playlist, errorRatio = getSongInfo(songs['id'].to_dict())\n",
    "save_pickle(\"data/playlist.p\", playlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df, _ = train_test_split(oneMSongs)\n",
    "train_df = musicInfo[['danceability', 'loudness', 'duration_ms']]\n",
    "train_df.head()\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dbscan = DBSCAN(eps = 70, min_samples = 100)\n",
    "clusters = dbscan.fit_predict(train_df)\n",
    "silhouette_score(train_df, clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Charistics related to each other\n",
    "\n",
    ":white_check_mark: `danceability` and `loudness`<br>\n",
    ":white_check_mark: `danceability` and `duration_ms`<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Song Charistiscs not related to each other\n",
    "\n",
    ":x: `tempo` and `energy`<br>\n",
    ":x: `danceability` and `energy`<br>\n",
    ":x: `danceability` and `popularity`<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.scatterplot(data=musicInfo, x='danceability', y='popularity')\n",
    "\n",
    "#plt.figure()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
